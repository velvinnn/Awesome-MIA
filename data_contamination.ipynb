{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "\n",
    "from file_paths import *\n",
    "import random\n",
    "\n",
    "model_and_data={\n",
    "    \"allenai/OLMo-2-1124-7B\":[AS_cpp, AS_python, AS_github_lean,AS_julia,AS_tex, AS_github_isabelle,AS_fortran, AS_github_coq, AS_r],\n",
    "    'HuggingFaceH4/zephyr-7b-beta':[UltraChat],\n",
    "    BioMistral:[RE_2012temporal,STS_STS_B,DC_MTSample,RE_2011coreference,events_BioRed, events_NLM_Gene,events_2012temporal,events_2006deid,events_BioASQ],\n",
    "    'EleutherAI/pythia-70m':[Github,FreeLaw,Enron_Emails,ArXiv,OpenWeb_Text2,Open_Subtitles,Hacker_News,YoutubeSubtitles,Pile_CC],\n",
    "    'EleutherAI/pythia-160m':[Github,FreeLaw,Enron_Emails,ArXiv,OpenWeb_Text2,Open_Subtitles,Hacker_News,YoutubeSubtitles,Pile_CC],\n",
    "    'EleutherAI/pythia-410m':[Github,FreeLaw,Enron_Emails,ArXiv,OpenWeb_Text2,Open_Subtitles,Hacker_News,YoutubeSubtitles,Pile_CC],\n",
    "    'EleutherAI/pythia-1.4b':[Github,FreeLaw,Enron_Emails,ArXiv,OpenWeb_Text2,Open_Subtitles,Hacker_News,YoutubeSubtitles,Pile_CC],\n",
    "    'EleutherAI/pythia-2.8b':[Github,FreeLaw,Enron_Emails,ArXiv,OpenWeb_Text2,Open_Subtitles,Hacker_News,YoutubeSubtitles,Pile_CC],\n",
    "    'EleutherAI/pythia-6.9b':[Github,FreeLaw,Enron_Emails,ArXiv,OpenWeb_Text2,Open_Subtitles,Hacker_News,YoutubeSubtitles,Pile_CC],\n",
    "    'EleutherAI/pythia-12b':[Github,FreeLaw,Enron_Emails,ArXiv,OpenWeb_Text2,Open_Subtitles,Hacker_News,YoutubeSubtitles,Pile_CC],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1 getting the perplexity, token ranks and token probabilities for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # specifying the GPU cores you want to use.\n",
    "os.environ[\"WORLD_SIZE\"] = \"2\"\n",
    "from probability_helper import *\n",
    "assert torch.cuda.is_available()\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for base_model_id in model_and_data:\n",
    "    model,tokenizer=load_model(base_model_id,device)\n",
    "    if 'BioMistral' in base_model_id:\n",
    "        model_name='BioMistral' \n",
    "    else:\n",
    "        model_name=base_model_id.split('/')[-1]\n",
    "    \n",
    "    outdir=output_path+\"/\"+model_name+'/pairs'\n",
    "    domains=model_and_data[base_model_id]\n",
    "    for domain in domains:\n",
    "        try:\n",
    "            train_file,test_file=domain\n",
    "            if 'BioMistral' in model_name:\n",
    "                domain_name='_'.join(train_file.split('/')[-3:-1]).replace('-','_')\n",
    "            else:\n",
    "                domain_name=train_file.split('/')[-1].replace('_train.jsonl','').replace('train.json','')\n",
    "            \n",
    "            for key,filename in [['unseen',test_file],['seen',train_file]]:\n",
    "                outname=f'{outdir}/{domain_name}_{key}.json'\n",
    "                if os.path.isfile(outname):\n",
    "                    continue\n",
    "                if 'jsonl' in filename:\n",
    "                    all_lines=open(filename,encoding='utf-8').readlines()\n",
    "                    if len(all_lines)>1101:\n",
    "                        random.seed(0)\n",
    "                        all_lines=random.sample(all_lines,1100)\n",
    "                    dics=[json.loads(line) for line in all_lines][:1000]\n",
    "                else:\n",
    "                    dics=json.loads(open(filename,encoding='utf-8').read())\n",
    "                    if len(dics)>1000:\n",
    "                        dics=random.sample(dics,1000)\n",
    "                output=[]\n",
    "                for dic in tqdm(dics):\n",
    "                    if 'text' in dic:\n",
    "                        text=dic[\"text\"]\n",
    "                    elif 'messages' in dic:\n",
    "                        text='\\n'.join([f'<{message[\"role\"]}> {message[\"content\"]} <{message[\"role\"]}/>' for message in dic['messages']])\n",
    "                        \n",
    "                    offset_mapping,all_tokens,ranks,token_probs,perplexity=find_ranks(text,base_model_id,model,tokenizer,device, start_token=1)\n",
    "                    output.append({\n",
    "                        'text':text,\n",
    "                        'tokens':all_tokens[:min(len(all_tokens),2048)],\n",
    "                        'ranks':ranks,\n",
    "                        'perplexity':perplexity,\n",
    "                        'token_probs':token_probs,\n",
    "                    })\n",
    "                    #break\n",
    "                os.makedirs(outdir,exist_ok=True)\n",
    "                with open(outname,'w') as f:\n",
    "                    json.dump(output,f,indent=4)\n",
    "        except:\n",
    "            print(f'{model}, {domain} failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 calculating AUCs and plot density plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_ps=[5, 10, 25]\n",
    "rank_thresholds=[1,3,5] \n",
    "token_thresholds=[5,15,25]\n",
    "ppl_keys=[f'ppl_{i}' for i in [50,100,200]]+['perplexity'] # \n",
    "rows=ppl_keys  +  [f'Min {k}% token' for k in token_thresholds] + [f'Mem {k}' for k in rank_thresholds] + [f'Entropy {k}' for k in entropy_ps] + ['PPL_seen','PPL_unseen']\n",
    "\n",
    "domain_map={\n",
    "    'RE_2012temporal':'RE 2012temp',\n",
    "    'STS_STS_B':'STS B',\n",
    "    'DC_MTSample':'DC MTSample', \n",
    "    'RE_2011coreference':'RE 2011coref',\n",
    "    'events_BioRed':'events BioRed', \n",
    "    'events_NLM_Gene':'events NLMGene', \n",
    "    'events_2012temporal':'events 2012temp', \n",
    "    'events_2006deid':'events 2006deid', \n",
    "    'events_BioASQ':'events BioASQ'\n",
    "}\n",
    "\n",
    "\n",
    "density_set=set(['Github','Pile-CC'])\n",
    "for model, files in model_and_data.items():\n",
    "    if 'pythia-6.9b' not in model:\n",
    "        continue\n",
    "    if 'BioMistral' in model:\n",
    "        domains=['_'.join(file[1].split('/')[-3:-1]).replace('-','_') for file in files]\n",
    "    else:\n",
    "        domains=[file[0].replace('/train.json','').replace('/evaluation.json','').split('/')[-1].split('_train')[0] for file in files]\n",
    "    model_tag=model.split('/')[-1]\n",
    "    outname=f'data/analysis/pairs/{model_tag}.json'\n",
    "    if not os.path.isfile(outname):\n",
    "        scores={}\n",
    "        for domain1 in tqdm(domains):\n",
    "            for domain2 in domains:\n",
    "                train_file=f'data/{model_tag}/pairs/{domain1}_seen.json'\n",
    "                test_file=f'data/{model_tag}/pairs/{domain2}_unseen.json'\n",
    "                \n",
    "                assert os.path.isfile(train_file),train_file\n",
    "                assert os.path.isfile(test_file),test_file\n",
    "                \n",
    "                train_domain=domain_map.get(domain1,domain1)\n",
    "                test_domain=domain_map.get(domain2,domain2)\n",
    "                \n",
    "                domain_set=set([train_domain,test_domain])\n",
    "                plot_figure=model_tag=='pythia-6.9b' and (density_set.issuperset(domain_set) or domain_set==density_set )\n",
    "                score=plot_and_calculate(train_file,test_file,train_domain,test_domain,model_tag,plot_figure=plot_figure)\n",
    "                scores[f'{domain1}[sep]{domain2}']=score\n",
    "        with open(outname,'w') as f:\n",
    "            json.dump(scores,f,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3 save results to latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving those information to latex\n",
    "for model, files in model_and_data.items():\n",
    "    if 'BioMistral' in model:\n",
    "        domains=['_'.join(file[1].split('/')[-3:-1]).replace('-','_') for file in files]\n",
    "    else:\n",
    "        domains=[file[0].replace('/train.json','').replace('/evaluation.json','').split('/')[-1].split('_train')[0] for file in files]\n",
    "    model_tag=model.split('/')[-1]\n",
    "    scores=json.load(open(f'data/analysis/pairs/{model_tag}.json'))\n",
    "    model_tag=model.split('/')[-1]\n",
    "    output=text_before\n",
    "    for domain in domains:\n",
    "        domain_tag=domain_map.get(domain,domain)\n",
    "        words=domain_tag.split(' ')\n",
    "        if len(words)==1:\n",
    "            output+='& \\\\textbf{'+domain_tag+'}'\n",
    "        else:\n",
    "            output+='\\\\textbf{\\\\begin{tabular}[c]{@{}c@{}}'+words[0]+'-\\\\\\\\ '+words[1]+'\\\\end{tabular}}'\n",
    "    output+='\\\\\\\\\\hline\\n'\n",
    "    for line_start, metric in latex_lines.items():\n",
    "        output+=line_start\n",
    "        for domain in domains:\n",
    "            score=scores[domain+\"[sep]\"+domain][metric]\n",
    "            if isinstance(score,list):\n",
    "                output+=f'& {round(score[0],1)}$\\\\pm${round(score[1],1)} '\n",
    "            else:\n",
    "                output+=f'& {round(score,1)} '\n",
    "        if line_start==' & Entropy 25':\n",
    "            output+='\\\\\\\\\\hline\\n\\\\multicolumn{2}{l}{\\\\textbf{Average AUC}}'\n",
    "            for domain in domains:\n",
    "                score=np.mean([scores[domain+\"[sep]\"+domain][m] for m in rows[:-2]])\n",
    "                output+=f'& {round(score,1)} '\n",
    "        output+='\\n'\n",
    "    if 'pythia' in model:\n",
    "        dataset='Pile'\n",
    "    elif 'mistral' in model.lower():\n",
    "        dataset='Medical-NLU'\n",
    "    else:\n",
    "        dataset='Algebraic Stack'\n",
    "    output+=get_text_after(model_tag,dataset)\n",
    "    with open(f'data/analysis/latex/{model_tag}.tex','w') as f:\n",
    "         f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4 cross-domain heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, files in model_and_data.items():\n",
    "    if len(files)!=9 or 'pythia-6.9b' not in model:\n",
    "        continue\n",
    "    \n",
    "    if 'BioMistral' in model:\n",
    "        domains=['_'.join(file[1].split('/')[-3:-1]).replace('-','_') for file in files]\n",
    "    else:\n",
    "        domains=[file[0].replace('/train.json','').replace('/evaluation.json','').split('/')[-1].split('_train')[0] for file in files]\n",
    "    \n",
    "    abbreviations=[domain_map.get(domain1,domain1).split(' ') for domain in domains]\n",
    "    abbreviations=[f'{word[0]}{word[1]}'.upper() if len(word)==2 else word[0][:4] for word in abbreviations]\n",
    "    model_tag=model.split('/')[-1]\n",
    "    model='6.9b'\n",
    "    scores=json.load(open(f'data/analysis/pairs/{model_tag}.json'))\n",
    "    for metric in rows[:-2]:\n",
    "        data=[]\n",
    "        for domain1 in domains: #seen\n",
    "            values=[scores[f'{domain1}[sep]{domain2}'][metric] for domain2 in domains]\n",
    "            data.append(values)\n",
    "        if 'PPL' in metric:\n",
    "            metric=metric.lower()\n",
    "        outname=f'data/analysis/plots/heats/{model}_{metric}.png'.replace('%','').replace(' ','_')\n",
    "        #summary.to_csv(outname+\".csv\")\n",
    "        plot_heat_map(data,abbreviations,abbreviations,outname)\n",
    "        #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
